{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7a15f98-12dc-409a-92f2-e00f118aba15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import subword_nmt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350305a9-0080-494a-baea-dbce69c4b3a1",
   "metadata": {},
   "source": [
    "# Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cae4ad16-471a-4882-afc5-b6551dc0d4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK tokenization data\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e3c33d-dcfe-4ea9-9dc5-8b1f68a7348e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['punkt', 'punkt.zip']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\Hi/nltk_data',\n",
       " 'C:\\\\Users\\\\Hi\\\\miniconda3\\\\envs\\\\py_3.11\\\\nltk_data',\n",
       " 'C:\\\\Users\\\\Hi\\\\miniconda3\\\\envs\\\\py_3.11\\\\share\\\\nltk_data',\n",
       " 'C:\\\\Users\\\\Hi\\\\miniconda3\\\\envs\\\\py_3.11\\\\lib\\\\nltk_data',\n",
       " 'C:\\\\Users\\\\Hi\\\\AppData\\\\Roaming\\\\nltk_data',\n",
       " 'C:\\\\nltk_data',\n",
       " 'D:\\\\nltk_data',\n",
       " 'E:\\\\nltk_data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(os.listdir(r'C:\\Users\\Hi\\AppData\\Roaming\\nltk_data\\tokenizers'))\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c29fd40-17a9-4a27-8cfd-0dfe7096b2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'love', 'tokenization', 'in', 'NLP', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Alexander',\n",
       " 'Rybak',\n",
       " '-',\n",
       " 'Fairytale',\n",
       " '(',\n",
       " 'LIVE',\n",
       " ')',\n",
       " '|',\n",
       " 'Norway',\n",
       " 'ðŸ‡³ðŸ‡´',\n",
       " '|',\n",
       " 'Grand',\n",
       " 'Final',\n",
       " '|',\n",
       " 'Winner',\n",
       " 'of',\n",
       " 'Eurovision',\n",
       " '2009']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I love tokenization in NLP.\"\n",
    "\n",
    "# Word-based tokenization\n",
    "tokens = word_tokenize(text, preserve_line=True)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "\n",
    "ytb_title = \"Alexander Rybak - Fairytale (LIVE) | Norway ðŸ‡³ðŸ‡´ | Grand Final | Winner of Eurovision 2009\"\n",
    "word_tokenize(ytb_title, preserve_line=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e4c8f3f-44ab-4549-a7a5-5d29df2f65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: ['I', ' ', 'l', 'o', 'v', 'e', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', ' ', 'i', 'n', ' ', 'N', 'L', 'P', '.']\n",
      "Character Vocabulary: {'n': 0, 'I': 1, 'N': 2, 'e': 3, 'v': 4, '.': 5, ' ': 6, 'L': 7, 'l': 8, 'k': 9, 'i': 10, 'P': 11, 'a': 12, 'o': 13, 'z': 14, 't': 15}\n",
      "Character Indices: tensor([ 1,  6,  8, 13,  4,  3,  6, 15, 13,  9,  3,  0, 10, 14, 12, 15, 10, 13,\n",
      "         0,  6, 10,  0,  6,  2,  7, 11,  5])\n"
     ]
    }
   ],
   "source": [
    "chars = list(text)\n",
    "print(f\"Characters: {chars}\")\n",
    "\n",
    "# Create a vocabulary for characters\n",
    "vocab_char = {char: i for i, char in enumerate(set(chars))}  # Character vocabulary\n",
    "print(f\"Character Vocabulary: {vocab_char}\")\n",
    "\n",
    "# Convert characters to indices\n",
    "indices_char = torch.tensor([vocab_char[char] for char in chars], dtype=torch.long)\n",
    "print(f\"Character Indices: {indices_char}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d66fe0-f892-4c8b-8442-668d19554d02",
   "metadata": {},
   "source": [
    "# Subword Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79827ea5-b331-4ff9-8b58-aa9d8f3e0e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: subword_nmt in c:\\users\\hi\\miniconda3\\envs\\py_3.11\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: mock in c:\\users\\hi\\miniconda3\\envs\\py_3.11\\lib\\site-packages (from subword_nmt) (5.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hi\\miniconda3\\envs\\py_3.11\\lib\\site-packages (from subword_nmt) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\hi\\miniconda3\\envs\\py_3.11\\lib\\site-packages (from tqdm->subword_nmt) (0.4.6)\n",
      "Downloading sentencepiece-0.2.0-cp311-cp311-win_amd64.whl (991 kB)\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/991.5 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 262.1/991.5 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 262.1/991.5 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 262.1/991.5 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 262.1/991.5 kB ? eta -:--:--\n",
      "   ---------- ----------------------------- 262.1/991.5 kB ? eta -:--:--\n",
      "   -------------------- ----------------- 524.3/991.5 kB 233.0 kB/s eta 0:00:03\n",
      "   -------------------- ----------------- 524.3/991.5 kB 233.0 kB/s eta 0:00:03\n",
      "   -------------------- ----------------- 524.3/991.5 kB 233.0 kB/s eta 0:00:03\n",
      "   ------------------------------ ------- 786.4/991.5 kB 297.0 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 786.4/991.5 kB 297.0 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 786.4/991.5 kB 297.0 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 786.4/991.5 kB 297.0 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 786.4/991.5 kB 297.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 991.5/991.5 kB 263.8 kB/s eta 0:00:00\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece subword_nmt "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27343450-af42-42eb-8309-2b95f7ee849e",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "476d23c1-c868-48e4-8e67-2c05a90bf951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "def train_bpe(corpus_path, vocab_size=5000, model_prefix=\"bpe_model\"):\n",
    "    \"\"\"\n",
    "    Train a BPE tokenizer using SentencePiece.\n",
    "    \n",
    "    Args:\n",
    "    - corpus_path (str): Path to the text corpus for training.\n",
    "    - vocab_size (int): Size of the vocabulary.\n",
    "    - model_prefix (str): Prefix for saving the model.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={corpus_path} --model_prefix={model_prefix} --vocab_size={vocab_size} --model_type=bpe\"\n",
    "    )\n",
    "\n",
    "def apply_bpe(corpus_path, model_path, output_path):\n",
    "    \"\"\"\n",
    "    Apply the trained BPE model to the corpus.\n",
    "    \n",
    "    Args:\n",
    "    - corpus_path (str): Path to the text corpus.\n",
    "    - model_path (str): Path to the trained BPE model.\n",
    "    - output_path (str): Output path for the tokenized text.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(model_path)\n",
    "    \n",
    "    with open(corpus_path, 'r') as f_in, open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            tokenized = sp.encode_as_pieces(line.strip())  # Tokenizes the line using BPE\n",
    "            f_out.write(' '.join(tokenized) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b0beaa6-334d-440a-b5d7-d067ca523989",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bpe('data.txt', vocab_size=465, model_prefix=\"bpe_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9811259-525d-4826-81f4-8e89027c2f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_bpe('data.txt', 'bpe_model.model', 'Byte Pair Encoding.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd914cc7-455a-4832-8c36-185d13b3bfaa",
   "metadata": {},
   "source": [
    "## Unigram Language Modeling Tokenization (UnigramLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9abcd71a-360a-4d0b-855e-b539bc2e1628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_unigram(corpus_path, vocab_size=5000, model_prefix=\"unigram_model\"):\n",
    "    \"\"\"\n",
    "    Train a UnigramLM tokenizer using SentencePiece.\n",
    "    \n",
    "    Args:\n",
    "    - corpus_path (str): Path to the text corpus for training.\n",
    "    - vocab_size (int): Size of the vocabulary.\n",
    "    - model_prefix (str): Prefix for saving the model.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={corpus_path} --model_prefix={model_prefix} --vocab_size={vocab_size} --model_type=unigram\"\n",
    "    )\n",
    "\n",
    "def apply_unigram(corpus_path, model_path, output_path):\n",
    "    \"\"\"\n",
    "    Apply the trained Unigram LM model to the corpus.\n",
    "    \n",
    "    Args:\n",
    "    - corpus_path (str): Path to the text corpus.\n",
    "    - model_path (str): Path to the trained Unigram LM model.\n",
    "    - output_path (str): Output path for the tokenized text.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(model_path)\n",
    "    \n",
    "    with open(corpus_path, 'r') as f_in, open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            tokenized = sp.encode_as_pieces(line.strip())  # Tokenizes the line using Unigram LM\n",
    "            f_out.write(' '.join(tokenized) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d683516b-64da-4b98-a321-d8f5d728f122",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unigram('data.txt', vocab_size=94, model_prefix=\"unigram_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fc56d45-3b88-4dcc-8b43-a9f2c8717f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_unigram('data.txt', 'unigram_model.model', 'Unigram Language Modeling Tokenization.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759640c6-1789-409c-8bda-872b014984cb",
   "metadata": {},
   "source": [
    "## WordPiece Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dedf28fb-87aa-49bd-9132-5ca9616f15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "70d411c7-d171-4910-a5eb-1e524e8bbbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
    "\n",
    "def train_wordpiece(corpus_path, vocab_size=5000, model_prefix=\"wordpiece_model\"):\n",
    "    # Initialize the WordPiece model\n",
    "    tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "    tokenizer.normalizer = NFD()\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "    # Define the trainer\n",
    "    trainer = WordPieceTrainer(vocab_size=vocab_size, min_frequency=2, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "    # Train the model\n",
    "    tokenizer.train([corpus_path], trainer)\n",
    "\n",
    "    # Save the trained model\n",
    "    tokenizer.save(f\"{model_prefix}.json\")\n",
    "\n",
    "def apply_wordpiece(corpus_path, model_path, output_path):\n",
    "    # Load the trained model\n",
    "    tokenizer = Tokenizer.from_file(model_path)\n",
    "\n",
    "    with open(corpus_path, 'r', encoding='utf-8') as f_in, open(output_path, 'w', encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            tokenized = tokenizer.encode(line.strip())  # Tokenizes the line using WordPiece\n",
    "            f_out.write(' '.join(tokenized.tokens) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93f426f2-69cd-4d32-93fb-89578360b566",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wordpiece('data.txt', vocab_size=5000, model_prefix=\"wordpiece_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33b00e08-6bc8-42c4-8206-cde4f5179117",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_wordpiece('data.txt', 'wordpiece_model.json', 'WordPiece Tokenization.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a76fd6-c3bb-48c6-a507-77bdeea2a190",
   "metadata": {},
   "source": [
    "##  Byte-Level BPE (BBPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "129e2c81-cbdf-48d0-ba9e-61e8adcee12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bbpe(corpus_path, vocab_size=5000, model_prefix=\"bbpe_model\"):\n",
    "    \"\"\"\n",
    "    Train a Byte-level BPE tokenizer using SentencePiece.\n",
    "    \n",
    "    Args:\n",
    "    - corpus_path (str): Path to the text corpus for training.\n",
    "    - vocab_size (int): Size of the vocabulary.\n",
    "    - model_prefix (str): Prefix for saving the model.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        f\"--input={corpus_path} --model_prefix={model_prefix} --vocab_size={vocab_size} --model_type=bpe --character_coverage=1.0\"\n",
    "    )\n",
    "\n",
    "def apply_bbpe(corpus_path, model_path, output_path):\n",
    "    \"\"\"\n",
    "    Apply the trained BBPE model to the corpus.\n",
    "    \n",
    "    Args:\n",
    "    - corpus_path (str): Path to the text corpus.\n",
    "    - model_path (str): Path to the trained BBPE model.\n",
    "    - output_path (str): Output path for the tokenized text.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.load(model_path)\n",
    "    \n",
    "    with open(corpus_path, 'r') as f_in, open(output_path, 'w',encoding='utf-8') as f_out:\n",
    "        for line in f_in:\n",
    "            tokenized = sp.encode_as_pieces(line.strip())  # Tokenizes the line using Byte-level BPE\n",
    "            f_out.write(' '.join(tokenized) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5388b2db-35f9-45a5-97e9-39f611a61d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bbpe('data.txt', vocab_size=465, model_prefix=\"bbpe_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "10434377-df64-4b4f-9f4a-45a83dce2755",
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_bbpe('data.txt', 'bbpe_model.model', 'Byte-Level BPE (BBPE).txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py_3.11)",
   "language": "python",
   "name": "py_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
